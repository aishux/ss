import org.apache.spark.sql.functions._
import spark.implicits._

// Load your table (Replace with actual table loading method)
val df = spark.table("your_table")

// Filter for LEVEL4_CODE = 'P9WP'
val filteredDf = df.filter($"LEVEL4_CODE" === "P9WP")

// Explode VU1_EMAILS and VW3_EMAILS into individual email rows
val explodedVU1 = filteredDf
  .withColumn("email", explode(split(col("VU1_EMAILS"), ", ")))
  .select($"LEVEL4_CODE", $"LEVEL5_CODE", $"VU1_EMAILS", $"email")

val explodedVW3 = filteredDf
  .withColumn("email", explode(split(col("VW3_EMAILS"), ", ")))
  .select($"LEVEL4_CODE", $"LEVEL5_CODE", $"VU1_EMAILS", $"email")

// Merge both exploded email lists
val mergedEmails = explodedVU1.union(explodedVW3)

// Aggregate to get distinct emails back into VW3_EMAILS
val finalDf = mergedEmails
  .groupBy("LEVEL4_CODE", "LEVEL5_CODE", "VU1_EMAILS")
  .agg(array_join(collect_set(col("email")), ", ").alias("VW3_EMAILS"))

// Show the result
finalDf.show(false)
